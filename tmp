MapReduce = Map / Reduce
-> 데이터 처리를 위한 프로그래밍 모델, 병행성 + 병렬처리 
	HDFS로 저장된, 클러스터의 각 머신에서 분산 데이터 처리 가능 : YARN 활용
Job : 클라이언트가 수행하는 작업의 기본단위
	입력 데이터, 맵리듀스 프로그램, 설정 정보로 구성
	하둡에서는 map task / reduce task로 나눠 실행. 각 task는 YARN을 이용해 스케줄링되고, 분산된 클러스터의 여러 노드에서 실행된다.
	Job의 입력을 Input split이라 부르는 고정 크기 조각으로 분리, 각 스플릿마다 하나의 map task를 생성하고 스플릿의 각 레코드를 사용자 정의 맵 함수로 처리한다.
맵 -> 리듀스 단계
	맵은 간단한 단위작업을 처리하는 작업(wordcount) / 리듀서는 분산 작업된 맵 작업의 결과물을 취합하는 작업
	맵 태스크 결과는 로컬 디스크 저장(결과가 리듀스가 최종 결과를 생성하기 위한 중간 결과물, 잡 완료시 맵의 결과는 버려짐)
	리듀스 태스크는 모든 매퍼의 출력 결과를 입력으로 받음. / 리듀스 결과는 안정성을 위해 HDFS에 저장
	다수 리듀스일 경우 맵 태스크는 리듀스 수만큼 파티션 생성, 각 파티션에 결과를 분배. 개별키에 속한 레코드는 한 곳에만 배치
	   
컴바이너 함수
	맵의 결과(=리듀스의 입력)를 전송하는 데이터양을 줄이는데 효과적(ex Max 함수)
하둡 스트리밍
	기존 자바말고 Ruby or Python 가능, 표준 입출력으로 자바와 동일하게 맵리듀스 가능
 
하둡 기초
분산 파일시스템 : 데이터를 단일 물리 머신에 저장하지 않고, 네트워크로 연결된 여러 머신에 나눠 저장한 스토리지를 관리하는 파일시스템
	네트워크 기반이므로 네트워크 프로그램의 복잡성, 일반적인 디스크 파일시스템보다 복잡
HDFS(하둡 분산 파일시스템) : 범용 하드웨어로 구성된 클러스터에서 실행되고 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일시스템.
	매우 큰 파일 + 스트리밍 방식 데이터 접근 + 범용 하드웨어
	블록의 개념
	한 번에 읽고 쓸 수 있는 데이터의 최대량, 단일 디스크 파일시스템은 블록 크기가 보통 수킬로바이트(디스크 블록크기는 512바이트)
	HDFS의 블록크기는 대략 128MB. 블록 크기보다 작은 데이터일 경우 전체 블록 크기에 해당하는 하위 디스크를 모두 점유하지 X(ex 128MB의 블록에서 1MB 파일을 저장할 때1MB 디스크만 사용)
	큰 이유는? 탐색 비용을 최소화하기 위함. 블록이 매우 크면 시작점을 탐색하는데 걸리는 시간을 줄일 수 있고 전송에 더 많은 시간을 할애할 수 있음.
HDSF 클러스터는 네임노드(마스터)와 여러 개의 데이터노드(워커)로 구성
	 
	네임노드는 파일시스템의 네임스페이스 관리, 파일시스템 트리와 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터 유지, 블록 위치 정보는 디스크에 영속 저장 X
	HDSF 클라이언트는 사용자를 대신해 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근. 
	데이터노드는 클라이언트 or 네임노드의 요청이 있을 때 블록을 저장하고 탐색, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고
	네임노드가 없으면 파일시스템이 동작 X 데이터노드에 블록이 저장되어 있지만 블록정보를 이용해 파일을 재구성할 수 X
	네임노드 장애복구를 위해 1) 파일시스템 메타데이터를 파일로 백업 2) 보조 네임노드 운영
	 
블록 캐싱
	데이터노드는 기본적으로 디스크에 저장된 블록을 읽지만, 자주 접근하는 블록은 오프힙 블록캐시라는 데이터노드 메모리에 명시적으로 캐싱할 수 있음.
	잡 스케줄러(맵리듀스, 스파크)는 블록이 캐싱된 데이터노드에서 태스크 실행되도록 할 수 있고, 블록 캐시의 장점을 이용하면 읽기 성능을 높일 수 있음.
HDFS 페더레이션(뭔소리야)
	네임노드는 파일시스템의 모든 파일과 각 블록에 대한 참조 정보를 메모리에서 관리
	파일이 매우 많은 대형 클러스터에서 메모리로 인한 확장성 문제 발생
	이를 해결하기 위해 HDFS 페더레이션(연합체) 지원
	각 네임노드가 파일시스템의 네임스페이스 일부를 나눠 관리하는 방식으로 새로운 네임노드 추가할 수 있음. = 디렉토리 단위로 네임노드를 구성
	각 네임노드는 네임스페이스 볼륨(네임스페이스의 메타데이터 구성), 블록 풀(네임스페이스에 포함된 파일의 전체 블록을 보관)을 관리.
	HDFS 페더레이션을 사용하면 파일, 디렉토리의 정보를 가지는 네임스페이스와 블록의 정보를 가지는 블록 풀을 각 네임노드가 독립적으로 관리합니다. 네임스페이스와 블록풀을 네임스페이스 볼륨이라하고 네임스페이스 볼륨은 독립적으로 관리되기 때문에 하나의 네임노드에 문제가 생겨도 다른 네임노드에 영향을 주지 X
	 

HDFS 고가용성
	네임노드는 단일 고장점(single point of failure), 장애가 발생하면 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 X
	장애를 복구하기위해 새로운 네임노드를 구동? 재가동에 많은 시간 소요
	2.x부터 고가용성 지원
	활성대기 상태로 설정된 한쌍의 네임노드 / 활성 네임노드에 장애가 발생하면 대기 네임노드가 역할을 이어받아 요청 처리
	NFS 필러 or QJM)
	QJM은 HDFS 전용 구현체, 고가용성 에디트 로그를 지원하기 위한 목적
	저널 노드 그룹에서 동작, 각 에디트 로그는 전체 저널노드에 동시에 쓰여짐(저널노드는 세개, 하나가 손상되어도 문제가 X)
	활성 네임노드에 장애가 발생하면 대기 네임노드는 수십초 이내에 기존 네임노드 대체 가능
	장애복구 컨트롤러 : 대기 네임노드를 활성화시키는 전환 작업
	단 하나의 네임노드만 활성 상태를 보장하기 위해 주키퍼를 이용. 각 네임노드는 경량의 장애복구 컨트롤러 프로세스로 네임노드의 장애를 감시하고 네임노드에 장애가 발생하면 장애복구 지시
세이프 모드
	HDFS의 세이프모드(safemode)는 데이터 노드를 수정할 수 없는 상태 입니다. 세이프 모드가 되면 데이터는 읽기 전용 상태가 되고, 데이터 추가와 수정이 불가능 하며 데이터 복제도 일어나지 않습니다. 관리자가 서버 운영 정비를 위해 세이프 모드를 설정 할 수도 있고, 네임노드에 문제가 생겨서 정상적인 동작을 할 수 없을 때 자동으로 세이프 모드로 전환됩니다.
명령행 인터페이스
	Hadoop fs ~ : fs는 hdfs의 쉘 명령어
하둡 파일시스템
	Org.apache.hadoop.fs.FileSystem : 클라이언트 인터페이스
	자바 API를 통해 하둡 파일시스템과 연동 가능
자바 인터페이스로 데이터 읽기
	하둡 URL / 파일시스템 API
	Java FileSystem 클래스 관련 여러가지… 책을 한번더 읽어보는게 좋겠다
	FileSystem API를 통해 데이터 읽기 / 파일 생성하기 위한 메서드(데이터 쓰기) / 디렉터리 생성
	FIleStatus 클래스로 파일길이, 블록크기, 복제, 수정시간, 소유권, 권한정보 등 메타데이터 탐색 가능
	listStatus 메서드로 특정 디렉터리의 내용 조회 가능
	globStatus : 단일 표현식으로 다중 파일 매칭하기
	PathFilter로 정규표현식 매칭
	Delete()로 파일, 디렉터리 영구 삭제
데이터 흐름 파악하기
	파일 읽기
	 
	네임노드에 파일이 보관된 블록 위치 요청
	네임노드가 블록 위치 반환
	각 데이터 노드에 파일 블록을 요청
	노드의 블록이 깨져 있으면 네임노드에 이를 통지하고 다른 블록 확인
	클라이언트가 데이터를 얻기 위해 데이터노드에 직접 접촉하고 네임노드는 각 블록에 적합한 데이터노드 안내
	파일쓰기
	 
	네임노드에 파일 정보를 전송하고, 파일의 블록을 써야할 노드 목록 요청
	네임노드가 파일을 저장할 목록 반환
	데이터 노드에 파일 쓰기 요청
	데이터 노드간 복제가 진행
Distcp
	병렬로 다량의 데이터를 HDFS로 복사하는 프로그램
	맵리듀스 잡으로 구현, 클러스터 전반에 걸쳐 병렬로 수행되는 맵 태스크를 이용해 복사 작업 진행(리듀서는 X)
	클러스터 불균형이 일어날 수 있음 : balancer 도구 사용

 
YARN(Yet Another Resource Negotiator) : 클러스터 리소스 관리 및 애플리케이션 라이프 사이클 관리를 위한 아키텍처
	클러스터의 자원을 요청하고 사용하기 위한 API 제공
	직접 사용 X, 대신 YARN이 내장된 분산 컴퓨팅 프레임워크에서 고수준 API 작성(자원 관리의 자세한 내용은 알 수 X)
	  : 애플리케이션 구동 방식
	자원 관리
	리소스 매니저(ResourceManager) / 노드매니저(NodeManager) 
	노드매니저는 클러스터의 각 노드마다 실행, 현재 노드의 자원 상태를 관리하고, 리소스매니저에 현재 자원 상태를 보고
	리소스매니저는 노드매니저로부터 전달받은 정보를 이용하여 클러스터 전체의 자원을 관리, 자원 사용 상태를 모니터링하고, 애플리케이션 마스터에서 자 자원을 요청하면 비어 있는 자원을 사용할 수 있도록 처리.
	자원을 분배하는 규칙을 설정하는 것이 스케줄러(Scheduler)입니다. 스케줄러에 설정된 규칙에 따라 자원을 효율적으로 분배합니다.
	수명 관리
	애플리케이션의 라이프 사이클 관리는 애플리케이션 마스터(Application Master)와 컨테이너(Container)를 이용하여 처리.
	클라이언트가 리소스 매니저에 애플리케이션을 제출하면, 리소스 매니저는 비어 있는 노드에서 애플리케이션 마스터를 실행합니다. 애플리케이션 마스터는 작업 실행을 위한 자원을 리소스 매니저에 요청하고, 자원을 할당 받아서 각 노드에 컨테이너를 실행하고, 실제 작업을 진행합니다. 컨테이너는 실제 작업이 실행되는 단위입니다. 컨테이너에서 작업이 종료되면 결과를 애플리케이션 마스터에게 알리고 애플리케이션 마스터는 모든 작업이 종료되면 리소스매니저에 알리고 자원을 해제합니다.
	다양한 애플리케이션
	 
	직접 작성보다는 기존 spark, tez 등을 사용하는게 낫다?
	스케줄러
	리소스 매니저는 클러스터 자원을 관리하고, 애플리케이션 마스터의 요청을 받아서 자원을 할당합니다. 자원 할당을 위한 정책을 스케줄러라고 합니다.
	 
	FIFO(First in First out) : 선입선출, 대형이 먼저 들어오면 나머지는 구동할 수 X, 효율성이 떨어진다(공유 클러스터에서는 비추천)
	Fair scheduler : 모든 잡의 자원을 동적으로 분배 
	동일하게 자원을 할당하기에, 각 사용자가 자신의 큐를 할당받는 방식, 자원은 사용자 사이에서만 균등하게 할당된다는 점
	페어 스케줄러는 트리 형태로 계층화된 큐를 선언하고, 큐별로 사용가능한 용량을 할당하여 자원을 관리합니다. 예를 들어 100G의 메모리 용량을 가지는 클러스터에서 A, B 두개의 큐에 각각 최저 자원(minResource) <10000 mb, 10vcores> 최대 자원(maxResource) <60000 mb, 30vcores>을 설정하고, 각 큐가 상황에 맞게 최대 60G까지의 메모리를 사용하게 설정할 수 있습니다. A 큐의 하위에 A_sub_1, A_sub_2와 같은 형태로 큐를 설정할 수도 있습니다.
	Capacity scheduler : 트리 형태로 큐를 선언하고 각 큐 별로 이용할 수 있는 자원의 용량을 정하여 주면 그 용량에 맞게 자원을 할당 – 용량을 지정하기 때문에 전체적인 효율성이 떨어짐
	각자 자원 할당 용량을 정해주는 식, 각 조직은 할당된 전용 큐를 가지고, 해당 큐의 내부에는 FIFO 방식 적용
	하나의 단일 잡은 해당 큐의 가용량을 넘지 못하지만, 큐 안에 다수의 잡이 존재하고 현재 적용할 수 있는 자원이 남는다면 여분 자원 할당 가능 -> 큐 탄력성
	커패시티 스케줄러를 설정할 때는 큐의 역활에 따라 용량을 잘 배분하고, 최대 사용 가능 용량(maximum-capacity)과 사용자 제한(user-limit-factor)을 이용하여 사용할 수 있는 자원의 용량을 제한해 주는 것이 중요

하둡 데이터 I/O(아직은 잘 모르겠으니 다음에 필요할 때 다시 참고)
	데이터 손상을 검출하기 위해 체크섬 계산(대충 뭔가를 비교하는거라 이해하면 되겠따)
	체크섬이 원본과 정확히 일치하지 않는다면 그 데이터는 손상된 것
	HDFS에서도 체크섬 검증이 이루어짐
	압축 관련 => 당장 중요하진 않은 내용 / 필요할때 참고하기
	직렬화(Serialization) – 역직렬화
	네트워크 전송을 위해 구조화된 ‘객체’를 바이트 스트림으로 전환하는 과정(JAVA)
	마찬가지로 당장은 필요하지 않은 내용
	파일 기반 데이터 구조 – SequenceFile 클래스 제공(바이너리 키-값 쌍에 대한 영속적 데이터 구조 제공)

 
Hive : 하둡 기반 데이터 웨어하우징 프레임워크(웨어하우징용 솔루션), RDB의 DB, table과 같은 형태로 HDFS에 저장된 데이터의 구조를 정의하는 방법을 제공하며, 이를 대상으로 SQL과 유사한 HiveQL 쿼리를 이용해 데이터를 조회 가능
	일반적으로 하이브는 사용자의 워크스테이션에서 실행되고, 쿼리는 일련의 맵리듀스 잡으로 변환되어 하둡 클러스터에서 구동
	하이브는 HDFS에 저장된 데이터(디렉터리/파일)에 구조(스키마)를 입히는 방식으로 데이터를 테이블로 구조화.(테이블 스키마와 같은 메타데이터는 메타스토어라 불리는 유에 저장됨)
	OVERWRITE : 해당 테이블의 디렉터리에 존재하는 모든 파일을 삭제하는 기능, 이걸 생략하면 새로운 파일은 단순히 테이블의 디렉터리에 추가.
	Hive는 hadoop과 마찬가지로, hive-site.xml 파일을 통해 환경 설정을 하게 됨.
	Hive는 기본적으로 실행 엔진으로 맵리듀스를 사용하지만, 실행 엔진으로 Tez를 사용해 실행할 수도 있음. + Spark를 지원하기도
	Tez, Spark는 맵리듀스보다 더 높은 성능과 유연성을 제공하는 DAG 엔진. Ex) 잡의 임시 출력을 HDFS에 저장하는 맵리듀스와 달리 tez, spark는 임시 출력을 로컬 디스크에 저장하거나 메모리에 저장하는 방식으로 복제 오버헤드를 피할 수 있음
	테이블
	하이브 테이블은 ‘저장된 데이터’와 ‘테이블에서 데이터의 레이아웃을 기술하는 관련 메타데이터’로 논리적으로 구성. 일반적으로는 HDFS에 데이터를 두고, 하이브는 RBDMS에 메타데이터를 저장
	테이블 생성시 하이브는 기본적으로 데이터를 직접 관리(하이브가 데이터를 자신이 관리하는 웨어하우스 디렉터리로 이동) / 사용자는 외부 테이블(external table)을 생성해 웨어하우스 디렉터리 외부의 데이터를 참조할 수도
	관리 테이블의 경우 데이터를 관리 테이블에 Load할 때 데이터는 하이브의 웨어하우스 디렉터리로 이동
	CREATE TABLE managed_table (dummy STRING);
	LOAD DATA INPATH ‘/~/’ INTO table managed_table; -> data.txt를 manged_table으로 이동
	DROP TABLE managed_table; -> DROP 구문으로 테이블을 제거하면 메타스토어와 함께 데이터도 삭제.
	외부 테이블의 경우 사용자가 데이터의 생성, 삭제를 직접 제어해야 함.
	CREATE EXTERNAL TABLE external_table (dummy STRING) LOCATION ‘/user/ /external_table’
	LOAD DATA LOAD DATA INPATH ‘/~/’ INTO table external_table;
	하이브는 데이터를 직접 관리할 필요 X, 웨어하우스 디렉터리로 이동 X
	삭제할 떄는 메타데이터만 삭제
	하이브에서만 데이터를 처리하면 관리, 다른 도구와 함께 사용하면 외부 테이블
	파티션과 버킷
	하이브는 테이블을 파티션으로 구조화 가능. 
	파티션 : 테이블의 데이터를 날짜와 같은 파티션 컬럼 값을 기반으로 큰 단위로 분할하는 방식 -> 데이터의 일부를 매우 빠르게 질의 가능
	다중 차원으로 파티션 가능(서브 파티션 추가)
	PARTITIONED BY를 통해 파티션 선언
	Ex) CREATE TABLE logs (ts BIGINT, line STRING) PARTITIONED BY (dt STRING, country STRING);
	LOAD DATA LOCAL INPATH ‘~’ INTO TABLE logs PARTITION (dt = ‘2001-01-01’, country = ‘GB’); -> 파티션 값을 명시적으로 지정해줘야 함
	테이블, 파티션은 효율적인 쿼리를 위해 데이터에 추가된 구조인 버킷으로 더욱 세분화 가능
	Ex) 사용자 ID를 기준으로 버킷을 생성하면 전체 사용자 중 무작위 데이터 샘플을 뽑아 사용자가 작성한 쿼리가 제대로 실행되는지 빠르게 평가 가능.
	버킷으로 구조화 하는 이유 
	1) 매우 효율적 쿼리가 가능 : 버킷팅은 테이블에 대한 추가 구조를 부여하고 이를 사용 가능. 특히 조인할 칼럼에 대한 버킷을 가진 두 테이블을 조인할 때 맵 조인을 구현하면 효율적
	2) 효율적인 샘플링에 유리 : 매우 큰 데이터셋을 대상으로 할 때 데이터셋의 일부만으로 쿼리 수행할 수 있다면 매우 편리
	CREATE OnTABLE bucketed_users (id INT, name STRING) CLUSTERD BY (id) INTO 4 BUCKETS;
	User의 id를 기준으로 버킷을 결정 -> 하이브는 user id의 값을 해싱하고, 그 값을 버킷 수로 나눈 나머지를 버킷의 번호로 선택 -> 버킷은 사용자의 무작위 집합을 포함하는 효과
	동일 방식으로 버킷된 두개의 테이블에 맵 조인을 수행하면 왼쪽 버킷을 처리하는 매퍼는 상응 행이 오른쪽 버킷에 있다는 걸 사전에 인지하기에 조인을 수행할 때 우측 테이블에 저장된 전체 데이터의 작은 일부만 추출.
	버킷에 포함된 데이터는 하나 이상의 컬럼으로 정렬 가능 -> 조인 시 효울적인 병합 정렬 가능하므로 맵 조인의 속도를 더 향상 가능
	CREATE TABLE bucketed_users (id INT, name STRING) CLUSTERED BY (id) SORTED BY (id ASC) INTO 4 BUCKETS;
	기존 테이블을 버킷팅하려 할 때
	먼저 hive.enforce.bucketing 속성을 true로 설정한 후 테이블 정의에 선언된 개수만큼 버킷을 생성하도록 하이브에 지시, 이후 다음 INSERT 명령 실행
	INSERT OVERWRITE TABLE bucketed_users SELECT * FROM users;
	
